# Машинное обучение "без учителя". Задача кластеризации. 

В практических примерах ниже показано:   

* как пользоваться инструментами визуального анализа для предварительной интерпретации кластеров 
* как проводить кластерный анализ 
* как строить прогноз принадлежности к кластерам новых наблюдений
* как оценивать точность кластеризации

*Модели*: иерархический и неиерархический кластерный анализ

*Данные*: load_breast_cancer

Данные в примере:
* **mean radius** - средний радиус
* **mean texture** - средняя текстура
* **mean perimeter** - средний периметр
* **mean area** - средняя площадь
* **mean smoothness** - средняя гладкость
* **mean compactness** - средняя компактность
* **mean concavity** - средняя вогнутость
* **mean concave points** - средние вогнутые точки
* **mean symmetry** - средняя симметрия
* **mean fractal dimension** - средняя фрактальная размерность
* **radius error** - ошибка радиуса
* **texture error** - ошибка текстуры
* **perimeter error** - ошибка периметра
* **area error** - ошибка области
* **smoothness error** - ошибка плавности
* **compactness error** - ошибка компактности
* **concavity error** - ошибка вогнутости
* **concave points error** - ошибка вогнутых точек
* **symmetry error** - ошибка симметрии
* **fractal dimension error** - ошибка фрактального измерения
* **worst radius** - наихудший радиус
* **worst texture** - худшая текстура
* **worst perimeter** - худший периметр
* **worst area** - худшая площадь
* **worst smoothness** - наихудшая гладкость
* **worst compactness** - худшая компактность
* **worst concavity** - наихудшая вогнутость
* **worst concave points** - наихудшие вогнутые точки
* **worst symmetry** - наихудшая симметрия
* **worst fractal dimension** - наихудшее фрактальная размерность

Задача состоит в разделении ирисов на группы в зависимости от показателей "mean radius", "mean fractal dimension".

![1](https://user-images.githubusercontent.com/94290501/225143509-8d059027-ebb0-47f4-a9e1-2ffff2b49e2c.jpg)

![2](https://user-images.githubusercontent.com/94290501/225143554-9bfb4146-be62-4bf1-b924-1f70e8686501.jpg)

В нашем примере наблюдения визуально никак не сгруппированы. Но можно предположить, что лучше разделить их на 3 группы. Тем не менее, рассмотрим метод для определения наилучшего количества кластеров.

## Определение оптимального количества кластеров для метода локтя

Здесь $w_{ij}$ = 1, если наблюдение относится к кластеру $j$ и 0 в противоположном случае, а $m_i$ - центроид кластера.

Одна из основных трудностей в обучении без учителя состоит в том, что мы не знаем точного ответа. В нашем наборе данных нет 
установленных данных о метках классов, поэтому для количественного определения качества кластеризации нам нужно использовать внутренние метрики - такие как внутрикластерная **SSE** (искажение или инерция):
$$SSE = \sum\limits_{i=1}^n\sum\limits_{j=1}^kw_{ij}(x_i - m_i)^2$$

Основываясь на внутрикластерной SSE, мы можем применить графический инструмент,
так называемый метод локтя, для оценки оптимального числа k кластеров для поставленной задачи. 
Интуитивно мы можем сказать, что если k увеличивается, то искажение уменьшается. 
Это вызвано тем, что образцы будут ближе к центроидам, которым они назначены. 
В основе метода локтя лежит идея, которая состоит в том, чтобы идентифицировать значение k в точке,
где искажение начинает увеличиваться быстрее всего, что станет понятнее, если мы построим график искажения для разных значений k.

![3](https://user-images.githubusercontent.com/94290501/225143625-49c5ff7e-dbfd-41fc-9b20-86a0b38eeac9.jpg)

Как видно на графике выше, локоть расположен в k = 4, что свидетельствует о том, что k = 4 является хорошим выбором для этого набора данных.

![4](https://user-images.githubusercontent.com/94290501/225143660-2ad5a81c-d410-4a10-8c62-a63334fb5a0a.jpg)

Тем не менее, визуальный анализ говорит о наличии 3 различных групп. Сделаем и такое построение:

![4 1](https://user-images.githubusercontent.com/94290501/225149161-42f0b7b5-6c13-4978-9449-a215d3489d2c.jpg)

Видим, что каждая группа точек покрашена в цвет соответствующего кластера, а центроиды расположены внутри множества точек. Тем не менее, попробуем оценить качество кластеризации в обоих вариантах.

## Количественная оценка качества кластеризации

Внутренняя метрика для оценки качества кластеризации (наряду с **SSE**) представлена силуэтным анализом
Силуэтный анализ может использоваться в качестве графического инструмента для построения графика меры плотности группировки образцов в кластерах. 
Чтобы вычислить силуэтный коэффициент наблюдения в нашем наборе данных, можно применить следующие три шага. 
1. Вычислить *внутриклассовую связность* $a_i$ как среднее расстояние между наблюдением и другими точками кластера
2. Вычислить *межкластерное разделение* $b_i$ от следующего ближайшего кластера как среднее расстояние между наблюдением $х_i$ и всеми наблюдениями в ближайшем кластере
3. Вычислить *силуэт* $s_i$:
$$s_i = (b_i - a_i)/\max(b_i,a_i).$$

$s_i$ принимает значения в диапазоне $[-1, 1]$. Идеальное совпадение - когда $s_i = 1.$

Посчитаем для k=3 кластеров:

![5](https://user-images.githubusercontent.com/94290501/225143705-430e58da-f1df-4f83-a749-60c92e150571.jpg)

Средний коэффициент силуэта весьма близок к 0.55 , что говорит о относительным качестве кластеризации.
Если силуэты зрительно значительно отличаются друг от друга по длине, то это является признаком *субоптимальной* кластеризации. Как правило, в этом случае центроиды кластеров стоят отдельно от множества точек кластера. 
Посчитаем средний силуэтный коэффициент для кластеризации $k=4$. 

Средний коэффициент силуэта для кластеризации $k=4$  --  0.515687459695231

Его значение немного ниже, чем в предыдущем случае. Формально $k=3$ - более оптимальное разбиение.

## Сравнение результатов на обучающей и тестовой выборке
Посмотрим, как прогнозировать новых наблюдений принадлежность к кластерам, построенным по обучающим данным. Сравним значения средних силуэтных коэффициентов.

В обучающей выборке - 80% исходных наблюдений.

Добавляем к выборке дополнительный показатель.

![6](https://user-images.githubusercontent.com/94290501/225143744-3f7741ec-32f0-403d-b27d-997a6044cacc.jpg)

![7](https://user-images.githubusercontent.com/94290501/225143770-5e9e60c5-2549-45c7-94f2-0b490805951c.jpg)

Как видно на следующем выше графике, заметных изменений не обнаружилось.

Обучаем алгоритм и считаем средний силуэтный коэффициент.

Средний коэффициент силуэта --  0.5503935213155396

Применяем модель к новым данным. Значение среднего силуэтного коэффициента незначительно ухудшилось.(Средний коэффициент силуэта --  0.5372665596070236)

#### Статистический анализ получившихся кластеров

![8](https://user-images.githubusercontent.com/94290501/225143833-7c677df8-1d04-47a2-898c-0ad9ced30577.jpg)

Визуально кластеры немного отличаются друг от друга. По диаграммам разброса видно, что все три кластера образуют не совсем плотное множество точек, разбиение нестрогое.

## Оценка точности кластеризации с помощью Acc, сравнив оценки с фактическим разбиением на группы.

Acc для выборки с двумя показателями(sepal length (cm) petal length (cm)):
1) для разбиения на 2 кластера (Accuracy: 0.14)
2) для разбиения на 3 кластера (Accuracy: 0.46)
3) для разбиения на 4 кластера (Accuracy: 0.43)

Из оценки кластеризации следует, что алгоритм (при разбиении на 3 кластера) достиг высокой точности в своей работе по сравнению с другими вариантами кластеризации.

Acc для выборки с тремя показателями(sepal length (cm) petal length (cm) sepal width (cm)):
1) для разбиения на 2 кластера (Accuracy: 0.85)
2) для разбиения на 3 кластера (Accuracy: 0.27)
3) для разбиения на 4 кластера (Accuracy: 0.02)

Из оценки кластеризации следует, что алгоритм (при разбиении на 2 кластера) достиг высокой точности в своей работе, указывая правильную категоризацию большинства объектов данных.


